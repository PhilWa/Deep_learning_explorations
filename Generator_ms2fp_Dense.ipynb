{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Generator_ms2fp_Dense.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PhilWa/Deep_learning_explorations/blob/master/Generator_ms2fp_Dense.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9TnJztDZGw-n"
      },
      "source": [
        "# Predicting MACCs from MS2 data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gwCn7PrMbqqO"
      },
      "source": [
        "To let colab run longer\n",
        "- https://medium.com/@shivamrawat_756/how-to-prevent-google-colab-from-disconnecting-717b88a128c0\n",
        "\n",
        "To perform hyperparameter search\n",
        "- https://maxpumperla.com/hyperas/\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgkfT918y0i6",
        "colab_type": "text"
      },
      "source": [
        "## Defining experimental variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hB1h6IctR720",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "experiment_name = \"something_light_ms2fp\"\n",
        "data_dir = \"/content/drive/My Drive/MS2 predictions/data/\"\n",
        "output_dir = \"/content/drive/My Drive/MS2 predictions/\"\n",
        "save_model = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jT501IcfsNH7",
        "colab_type": "text"
      },
      "source": [
        "The aim of this setup is to train a model that beats Sirius4.\n",
        "Therefor different experiments will be performed. <br>\n",
        "\n",
        "The logic will be: Iterate over model structures (currently it looks like the bigger the model the better it performs), each with different data generators.\n",
        "Training will happen on 90% of the data. 10% will be validation data to ensure there is no overfitting. After each run (100 epochs) or if validation loss doesnt descrease in 5 consecutive runs.\n",
        "\n",
        "Then we compare the prediction to the output from Sirius4 from the CASMI challenges.\n",
        "\n",
        "To this end the hamming distance between:\n",
        "  - ground truth and Sirius4 fingerprints = sirius_Ham\n",
        "  - ground truht and current model finger print = swiss_Ham\n",
        "\n",
        "> if sirius_Ham > swiss_Ham <br>\n",
        "> print(\"Happy!\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_M7E6VQPy6gP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## add a .txt to add more meta info "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z41B-W81JYei"
      },
      "source": [
        "## Setup environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "z682XYsrjkY9",
        "colab": {}
      },
      "source": [
        " from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "!pip install tensorflow-gpu==2.0.0-beta1\n",
        "#import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import datetime, os\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-adZVBQNsaR",
        "colab_type": "text"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bu2yxLcNsaT",
        "colab_type": "text"
      },
      "source": [
        "- `smart_train_split` Function to split data in a way that there are only never before seen finger prints in the test data set\n",
        "- `compute_sample_weights` Function to weigh samples by the number of their occurance in the training data set\n",
        "- `compute_class_weights` Function to weigh classes by the number of their occurance in the training data set\n",
        "- `binarize_pred`\n",
        "- `evaluate_prediction`\n",
        "- `generate_ms2fp_data`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0Jkc6lSO9bSH",
        "colab": {}
      },
      "source": [
        "def smart_train_split(ms2_infos, finger_prints,random_seed, train_percentage, validation_fraction):\n",
        "    np.random.seed(random_seed)\n",
        "    samples_ids = finger_prints['duplicated_id'].values\n",
        "    unique_sample_ids = np.unique(samples_ids)\n",
        "    max_for_split = int(round(len(unique_sample_ids) * train_percentage))\n",
        "    print(max_for_split,len(unique_sample_ids), train_percentage)\n",
        "    sample_selection = np.random.choice(unique_sample_ids, size=max_for_split)\n",
        "    selection_bool = np.isin(samples_ids, sample_selection)\n",
        "\n",
        "    print(\"Train/Test Split\")\n",
        "    test_y = finger_prints[selection_bool]\n",
        "    train_temp_y = finger_prints[np.invert(selection_bool)]\n",
        "\n",
        "    test_x = ms2_infos[selection_bool]\n",
        "    train_temp_x = ms2_infos[np.invert(selection_bool)]\n",
        "    print(\"Train/Test Split - Done\")\n",
        "\n",
        "    print(\"Validation/Train Split\")\n",
        "\n",
        "    validation_sample_ids = samples_ids[np.invert(selection_bool)]\n",
        "    unique_validation_ids = np.unique(validation_sample_ids)\n",
        "    max_for_validation_split = int(round(len(unique_validation_ids)*validation_fraction))\n",
        "    validation_selection = np.random.choice(unique_validation_ids, size = max_for_validation_split)\n",
        "    validation_bool = np.isin(validation_sample_ids, validation_selection)\n",
        "\n",
        "    validation_y = train_temp_y[validation_bool]\n",
        "    train_y = train_temp_y[np.invert(validation_bool)]\n",
        "\n",
        "    validation_x = train_temp_x[validation_bool]\n",
        "    train_x = train_temp_x[np.invert(validation_bool)]\n",
        "\n",
        "    print(\"Validation/Train Split - Done\")\n",
        "    print(\"______________________________\")\n",
        "    print(train_y.shape, \"train_y\")\n",
        "    print(train_x.shape, \"train_x\")\n",
        "    print(test_y.shape, \"test_y\")\n",
        "    print(test_x.shape, \"test_x\")\n",
        "    print(validation_y.shape, \"validation_y\")\n",
        "    print(validation_x.shape, \"validation_x\")\n",
        "    \n",
        "    return train_x, test_x, validation_x, train_y, test_y, validation_y\n",
        "\n",
        "\n",
        "def compute_sample_weights(y_,plot_hist=False):\n",
        "    duplicated_ids=y_[\"duplicated_id\"]\n",
        "    #Compute sample_weight_vector\n",
        "    from collections import Counter\n",
        "    cnt = Counter()\n",
        "    for occurance in duplicated_ids:\n",
        "        cnt[occurance] += 1\n",
        "    final = Counter(cnt)\n",
        "    \n",
        "    freq_collection = pd.DataFrame.from_dict(final, orient=\"index\").reset_index()\n",
        "    freq_collection = freq_collection.rename(columns={\"index\":\"Duplicated_id\",0:\"Frequency\"})\n",
        "    freq_collection[\"Duplicated_id\"] = freq_collection[\"Duplicated_id\"].apply(str)\n",
        "\n",
        "    duplicated_ids = pd.DataFrame(duplicated_ids)\n",
        "    y_ids = duplicated_ids.rename(columns={\"duplicated_id\":\"Duplicated_id\"})\n",
        "    y_ids[\"Duplicated_id\"] = y_ids[\"Duplicated_id\"].apply(str)\n",
        "\n",
        "    y_ids = y_ids.merge(freq_collection,on=\"Duplicated_id\", suffixes=('_y_ids','_freq_ids'))\n",
        "    sample_weight_vector = y_ids[\"Frequency\"]\n",
        "    if plot_hist:\n",
        "      plt.hist(sample_weight_vector)\n",
        "    \n",
        "    return sample_weight_vector\n",
        "\n",
        "def compute_class_weights(y_, plot_hist=False):\n",
        "    y_clean = y_.drop(\"duplicated_id\",axis=1)\n",
        "    equal_one = (y_clean==1).sum()\n",
        "    class_weight_vector=(max(equal_one)/equal_one)\n",
        "    inf_bool = np.isinf(class_weight_vector)\n",
        "    class_weight_vector[inf_bool]=0\n",
        "    if plot_hist:\n",
        "      plt.hist(class_weight_vector, bins=40)\n",
        "    \n",
        "    return class_weight_vector\n",
        "\n",
        "\n",
        "\n",
        "from progressbar import *               # just a simple progress bar\n",
        "\n",
        "widgets = ['Progess: ', Percentage(), ' ', Bar(marker='#',left='[',right=']'),\n",
        "           ' ', ETA(), ' ', FileTransferSpeed()] #see docs for other options\n",
        "\n",
        "# pbar = ProgressBar(widgets=widgets, maxval=500)\n",
        "# pbar.start()\n",
        "\n",
        "# for i in range(100,500+1,50):\n",
        "#     # here do something long at each iteration\n",
        "#     pbar.update(i) #this adds a little symbol at each iteration\n",
        "# pbar.finish()\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "def binarize_pred(pred_fp, cutoff_value = 0.5):\n",
        "    pred_fp=pd.DataFrame(pred_fp)\n",
        "    pred_fp[pred_fp<cutoff_value]=0\n",
        "    pred_fp[pred_fp>=cutoff_value]=1\n",
        "    return pred_fp\n",
        "\n",
        "\n",
        "def evaluate_prediction(test_Y_clean, predicted_Y_df, return_hamming_distance = False, save_output = False):\n",
        "    from scipy.spatial import distance\n",
        "    hamming_pred = []\n",
        "    hamming_rand = []\n",
        "    hamming_superrand = []\n",
        "    pbar = ProgressBar(widgets=widgets, maxval=test_Y_clean.shape[0])\n",
        "    pbar.start()\n",
        "\n",
        "    for index, fingerprint in test_Y_clean.iterrows():\n",
        "      predicted_fingerprint = predicted_Y_df.iloc[index]\n",
        "      random_int = np.random.randint(0,test_Y_clean.shape[0])\n",
        "      random_predicted_fingerprint = predicted_Y_df.iloc[random_int]\n",
        "      hamming_pred.append(distance.hamming(fingerprint,predicted_fingerprint))\n",
        "      hamming_rand.append(distance.hamming(fingerprint,random_predicted_fingerprint))\n",
        "      hamming_superrand.append(distance.hamming(np.random.randint(0,2, 152),np.random.randint(0,2, 152)))\n",
        "      pbar.update(index)\n",
        "\n",
        "    pbar.finish()\n",
        "\n",
        "    median_distance = np.median(hamming_rand)-np.median(hamming_pred)\n",
        "    plt.hist(hamming_pred, alpha=0.4,color='B', label='Predicted Fingerprint')\n",
        "    plt.hist(hamming_rand, alpha=0.4,color='R', label='Random Fingerprint')\n",
        "    plt.hist(hamming_superrand, alpha=0.4,color='G', label='Random binary vector')\n",
        "    plt.title('Hamming distance ' + experiment_name + \" avg:s \"+str(np.round(median_distance, 3)))\n",
        "    plt.axvline(np.median(hamming_pred), color='B', linestyle=':', linewidth=1)\n",
        "    plt.axvline(np.median(hamming_rand), color='R', linestyle='dashed', linewidth=1)\n",
        "    plt.axvline(np.median(hamming_superrand), color='G', linestyle='dashed', linewidth=1)\n",
        "    plt.legend(loc='upper right')\n",
        "    if save_output: \n",
        "      filepath=data_dir + dt_string + \"_\" + experiment_name + \"hamming_distribution\" + \".png\"\n",
        "      plt.savefig(filepath)\n",
        "    plt.show()\n",
        "    \n",
        "    \n",
        "    print(np.mean(median_distance))\n",
        "    if return_hamming_distance:\n",
        "      return hamming_pred\n",
        "\n",
        "def generate_ms2fp_data(ms2_info,\n",
        "                        finger_prints,\n",
        "                        batch_size = 100,\n",
        "                        flip= True,\n",
        "                        drop_peaks = True,\n",
        "                        output_shape = \"Conv1D\" ,\n",
        "                        generator_mode = \"Train\"):\n",
        "    i = 0\n",
        "    ms2_info = ms2_info.values\n",
        "    finger_prints = finger_prints.values\n",
        "    while True:\n",
        "        ms2_info_out = np.zeros((batch_size,ms2_info.shape[1]))\n",
        "        finger_prints_out = np.zeros((batch_size,finger_prints.shape[1]))\n",
        "        for b in range(batch_size):\n",
        "            if i == ms2_info.shape[0]:\n",
        "                i = 0\n",
        "                \n",
        "                shuffle_numbers = np.random.randint(0,ms2_info.shape[0],ms2_info.shape[0])\n",
        "                ms2_info = ms2_info[shuffle_numbers,:]\n",
        "                finger_prints = finger_prints[shuffle_numbers,:] \n",
        "            \n",
        "            if generator_mode == \"Train\":\n",
        "                meta_info = ms2_info[i,:3]\n",
        "                noised_ms = ms2_info[i,3:] * np.random.normal(1,0.1,1)\n",
        "                \n",
        "                if drop_peaks:\n",
        "                    ms_info = np.concatenate([meta_info, noised_ms])\n",
        "                    non_zeros = np.asarray(np.nonzero(ms_info[3:]))[0]\n",
        "                    zero_id = np.random.randint(3, len(non_zeros),int(len(non_zeros)*0.1)) \n",
        "                    ms_info_dropped = ms_info[zero_id] = 0\n",
        "                    ms2_info_out[b,3:] = ms_info_dropped\n",
        "                else:\n",
        "                    ms2_info_out[b,:] = np.concatenate([meta_info, noised_ms])\n",
        "\n",
        "                finger_prints_out[b,:] = finger_prints[i,:]\n",
        "            if generator_mode == \"Validate\":\n",
        "                \n",
        "                finger_prints_out[b,:] = finger_prints[i,:]\n",
        "                ms2_info_out[b,:] = ms2_info[i,:]\n",
        "            \n",
        "            i += 1\n",
        "        \n",
        "        if flip & (generator_mode == \"Train\"):\n",
        "            ms2_info_out[1::2, :] = ms2_info_out[1::2, ::-1]\n",
        "        \n",
        "        if output_shape == \"Conv1D\":\n",
        "            ms2_info_out =  np.reshape(ms2_info_out, (ms2_info_out.shape[0],ms2_info_out.shape[1],1))\n",
        "\n",
        "        yield (ms2_info_out, finger_prints_out) \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "47mEH4pS-rrC"
      },
      "source": [
        "## Load and prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7NCX6qGgv7Hh",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Oivh2ohhytpO",
        "colab": {}
      },
      "source": [
        "y_in=pd.read_pickle(data_dir + \"y_in.pkl\")\n",
        "x_in=pd.read_pickle(data_dir + \"x_in.pkl\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yd0CrSEvIvUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVlNKy15h5Xw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "y_colnames=y_in.columns.values.tolist()\n",
        "y_colnames_clean=[re.sub(\"MACC\", \"\", x) for x in y_colnames]\n",
        "\n",
        "y_colnames_clean = y_colnames_clean[:-1]\n",
        "y_colnames_clean = [int(i) for i in y_colnames_clean] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qI8bet7sp0la",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "empty_y=np.zeros((y_in.shape[0],166))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQNaTyA5pWb4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "empty_y[:,y_colnames_clean]="
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxMB6prdUWbu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "casmi_groundtruth_y = pd.read_csv(data_dir + \"calculated_maccs_casmi.csv\") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCmw2wNwhgTQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "casmi_groundtruth_y = casmi_groundtruth_y.drop(\"Unnamed: 0\", axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyKJJphRpPBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCcTQOoqh1oH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "casmi_groundtruth_y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vul-7P92ZxxF"
      },
      "source": [
        "Double check that the way the weight_vectors are designed on the splitted data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nbQMbTkA-pd5",
        "colab": {}
      },
      "source": [
        "train_X, test_X, validation_X, train_Y, test_Y, validation_Y  =  smart_train_split(x_in ,y_in , 41, 0.2, 0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9FrlXh1BnW4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del(y_in)\n",
        "del(x_in)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nezomxm_KOE-",
        "colab": {}
      },
      "source": [
        "sample_weights = compute_sample_weights(train_Y,plot_hist=False)\n",
        "validation_sample_weights =  compute_sample_weights(validation_Y,plot_hist=False)\n",
        "class_weights = compute_class_weights(train_Y,plot_hist=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1ibaSYa02BX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_Y = test_Y.drop(\"duplicated_id\",axis=1)\n",
        "train_Y = train_Y.drop(\"duplicated_id\",axis=1)\n",
        "validation_Y = validation_Y.drop(\"duplicated_id\", axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kDJAPdrKYIXa"
      },
      "source": [
        "## Feature curation \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "luuf7d4mrTL7"
      },
      "source": [
        "## Setup 1Dconv model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q6eQkwkI3CDg",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv1D(input_shape=(4444,1), kernel_size=32,filters=512, activation=\"relu\"),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dropout(0.25),\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.25),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.25),\n",
        "    tf.keras.layers.Dense(152, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# checkpoint to save best model\n",
        "from datetime import datetime\n",
        "# datetime object containing current date and time\n",
        "now = datetime.now()\n",
        "dt_string = now.strftime(\"%d_%m_%Y\")\n",
        "\n",
        "filepath= data_dir + dt_string + \"_\" + experiment_name + \"model_visualization\" + \".png\"\n",
        "tf.keras.utils.plot_model(model, to_file=filepath, show_shapes =True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcaVeIVkNsa6",
        "colab_type": "text"
      },
      "source": [
        "### Set up callback list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UJU8XNxNsa7",
        "colab_type": "text"
      },
      "source": [
        "Generate callback_lists to:\n",
        "- Save best model\n",
        "- Reduce learning rate upon val_loss plateau\n",
        "- "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V71aFJNSNsa8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "filepath=data_dir + dt_string + experiment_name + \".hdf5\"\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "# Adjust learning rate upon plateau\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                              patience=4, min_lr=0.001)\n",
        "\n",
        "callbacks_list = [reduce_lr]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BvAucO1_rNE_"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms75iMnh0kye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_generator = generate_ms2fp_data(train_X,train_Y, 100,drop_peaks=False, generator_mode = \"Train\")\n",
        "validation_generator = generate_ms2fp_data(validation_X,validation_Y, 100, generator_mode = \"Validation\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bEOq6sS85JKN",
        "colab": {}
      },
      "source": [
        "history = model.fit_generator(train_generator,\n",
        "                              steps_per_epoch=1000,\n",
        "                              validation_data = validation_generator, \n",
        "                              validation_steps = 10,\n",
        "                              epochs=3,\n",
        "                              verbose=1, callbacks=callbacks_list)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "fit(x=train_1D_X,\n",
        "            y=np.abs(train_Y),\n",
        "            validation_data = (validation_1D_X, np.abs(validation_Y), validation_sample_weights),\n",
        "            shuffle=True,\n",
        "            epochs=5,\n",
        "            batch_size=100,\n",
        "            class_weight=class_weights,\n",
        "            sample_weight=sample_weights,\n",
        "            callbacks = callbacks_list)\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2kSwbggpu6UM"
      },
      "source": [
        "## Investigate output of model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4FjhQ0Bn9uA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "last_trained_model = model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Od9d5ILP1UK-",
        "colab": {}
      },
      "source": [
        "test_1D_X = np.abs(test_X).values.reshape(test_X.shape[0],test_X.shape[1],1)\n",
        "#model.evaluate(x=np.abs(test_1D_X), y=np.abs(test_Y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWp9SweKnb-Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model_dir = data_dir + dt_string + experiment_name + \".hdf5\"\n",
        "#model = tf.keras.models.load_model(model_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NotnfZaGMxX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss ' + experiment_name)\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "filepath=data_dir + dt_string + \"_\" + experiment_name + \"loss_plot\" + \".png\"\n",
        "plt.savefig(filepath)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPO9gbMjODvi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy ' + experiment_name)\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "filepath=data_dir + dt_string + \"_\" + experiment_name + \"accuracy_plot\" + \".png\"\n",
        "plt.savefig(filepath)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_l3g_f11MQzz",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-1FOVk_P2VG5",
        "colab": {}
      },
      "source": [
        "predicted_Y = model.predict(x=np.abs(test_1D_X))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giyc1dHQId3C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_cut_Y = binarize_pred(predicted_Y,cutoff_value=.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A3CZO3Hf_9CN",
        "colab": {}
      },
      "source": [
        "tested_Y = test_Y.reset_index().drop([\"index\"], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWBNvmhPNsbV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluate_prediction(tested_Y, predicted_cut_Y, save_output= True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJvvJct4WeXJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "difference_matrix =(predicted_cut_Y.astype(\"int\").values==tested_Y.values).astype(\"int\")\n",
        "range_plot = range(difference_matrix.shape[0])\n",
        "\n",
        "difference_count = difference_matrix.shape[1]-np.sum(difference_matrix, axis=1)\n",
        "data = {\"Count\":difference_count, \"Sample_id\":range(difference_matrix.shape[0])}\n",
        "\n",
        "fig, ax = plt.subplots(1,5)\n",
        "sns.heatmap(tested_Y,cbar=False, xticklabels=False, yticklabels=False, ax=ax[0]).set_title('Y_test')\n",
        "sns.heatmap(predicted_cut_Y,cbar=False, xticklabels=False, yticklabels=False, ax=ax[1]).set_title('Y_pred')\n",
        "sns.heatmap(difference_matrix, xticklabels=False, yticklabels=False, ax=ax[2]).set_title('1-Diff')\n",
        "\n",
        "fig.tight_layout()\n",
        "fig.show()\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OX80lD32PT4h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_Y=test_Y.reset_index(drop=True)\n",
        "rowsum = np.sum(test_Y, axis = 1)\n",
        "colsum = np.sum(test_Y, axis = 0)\n",
        "plt.stem(rowsum, use_line_collection=True, markerfmt=\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmsngNtOPggc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.stem(colsum, use_line_collection=True, markerfmt=\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvyzLjF5JIUI",
        "colab_type": "text"
      },
      "source": [
        "## Save output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HwkkQ5JKUDZ",
        "colab_type": "text"
      },
      "source": [
        "### Save predictions as csv\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4zK4zs1S_J9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath=data_dir + dt_string + experiment_name + \"y_pred\" + \".csv\"\n",
        "predicted_Y.to_csv(filepath)\n",
        "filepath=data_dir + dt_string + experiment_name + \"y_test\" + \".csv\"\n",
        "test_Y.to_csv(filepath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AY9pLJvQKYz_",
        "colab_type": "text"
      },
      "source": [
        "Save "
      ]
    }
  ]
}